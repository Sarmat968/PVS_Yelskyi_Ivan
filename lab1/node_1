########################################
# JAVA=/usr/bin/java
# JAVA_OPTS=--add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED -Dlog4j.shutdownHookEnabled=false -Dhazelcast.logging.shutdown=true -Dhazelcast.logging.type=log4j2 -Dlog4j.configurationFile=file:/opt/hazelcast/config/log4j2.properties -Dhazelcast.config=/opt/hazelcast/config/hazelcast-docker.xml -Djet.custom.lib.dir=/opt/hazelcast/custom-lib -Djava.net.preferIPv4Stack=true -XX:MaxRAMPercentage=80.0
# CLASSPATH=/opt/hazelcast/*:/opt/hazelcast/lib:/opt/hazelcast/lib/*:/opt/hazelcast/bin/user-lib:/opt/hazelcast/bin/user-lib/*
########################################
2024-12-12 14:09:01,888 [ INFO] [main] [c.h.i.c.AbstractConfigLocator]: Loading configuration '/opt/hazelcast/config/hazelcast-docker.xml' from System property 'hazelcast.config'
2024-12-12 14:09:01,890 [ INFO] [main] [c.h.i.c.AbstractConfigLocator]: Using configuration file at /opt/hazelcast/config/hazelcast-docker.xml
2024-12-12 14:09:01,982 [ WARN] [main] [c.h.c.AbstractXmlConfigHelper]: Name of the hazelcast schema location[http://www.hazelcast.com/schema/config/hazelcast-config-5.3.xsd] is incorrect, using default
2024-12-12 14:09:02,246 [ INFO] [main] [c.h.i.c.o.ExternalConfigurationOverride]: Detected external configuration entries in environment variables: [hazelcast.network.publicaddress=192.168.0.104:5701,hazelcast.clustername=custom_cluster]
2024-12-12 14:09:02,292 [ INFO] [main] [c.h.i.AddressPicker]: [LOCAL] [custom_cluster] [5.4.0] Using public address: [192.168.0.104]:5701
2024-12-12 14:09:02,335 [ INFO] [main] [c.h.s.logo]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 
        o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
        |    |    / \       /         |     /         / \    |         |   
        o----o       o     o   o----o |    o             o   o----o    |   
        |    |  *     \   /           |     \       *     \       |    |   
        o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2024-12-12 14:09:02,336 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2024-12-12 14:09:02,338 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [192.168.0.104]:5701
2024-12-12 14:09:02,338 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Cluster name: custom_cluster
2024-12-12 14:09:02,339 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2024-12-12 14:09:02,348 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Jet is enabled
2024-12-12 14:09:02,964 [ INFO] [main] [c.h.s.security]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see ðŸ”’ security recommendations and the status of current config.
2024-12-12 14:09:03,033 [ INFO] [main] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Using Multicast discovery
2024-12-12 14:09:03,036 [ WARN] [main] [c.h.c.CPSubsystem]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2024-12-12 14:09:03,301 [ INFO] [main] [c.h.j.i.JetServiceBackend]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Setting number of cooperative threads and default parallelism to 4
2024-12-12 14:09:03,645 [ INFO] [main] [c.h.i.d.Diagnostics]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2024-12-12 14:09:03,651 [ INFO] [main] [c.h.c.LifecycleService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] [192.168.0.104]:5701 is STARTING
2024-12-12 14:09:06,161 [ INFO] [main] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:1, ver:1} [
        Member [192.168.0.104]:5701 - 9704bf18-1cec-4db4-8d98-a9a5f55fdf00 this
]

2024-12-12 14:09:06,168 [ INFO] [main] [c.h.j.i.JobCoordinationService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Jet started scanning for jobs
2024-12-12 14:09:06,170 [ INFO] [main] [c.h.c.LifecycleService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] [192.168.0.104]:5701 is STARTED
2024-12-12 14:10:32,175 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initialized new cluster connection between /172.19.0.3:5701 and /172.19.0.1:38174
2024-12-12 14:10:32,263 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:2, ver:2} [
        Member [192.168.0.104]:5701 - 9704bf18-1cec-4db4-8d98-a9a5f55fdf00 this
        Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf
]

2024-12-12 14:10:32,327 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.i.p.i.PartitionStateManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initializing cluster partition table arrangement...
2024-12-12 14:11:10,104 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initialized new cluster connection between /172.19.0.3:5701 and /172.19.0.1:58304
2024-12-12 14:11:10,196 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:3, ver:3} [
        Member [192.168.0.104]:5701 - 9704bf18-1cec-4db4-8d98-a9a5f55fdf00 this
        Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf
        Member [192.168.0.104]:5703 - 51bbddca-3e20-4ed9-aec3-8afedc4a147b
]

2024-12-12 14:11:10,460 [ INFO] [hz.festive_germain.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Repartitioning cluster data. Migration tasks count: 271
2024-12-12 14:11:11,145 [ INFO] [hz.festive_germain.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] All migration tasks have been completed. (repartitionTime=Thu Dec 12 14:11:10 GMT 2024, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=271)
2024-12-12 14:12:16,690 [ INFO] [hz.festive_germain.generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=3, /172.19.0.3:5701->/172.19.0.1:40942, qualifier=null, endpoint=[172.19.0.1]:40942, remoteUuid=aeae1c26-9aad-41cc-a7f5-35a68546a49a, alive=true, connectionType=MCJVM, planeIndex=-1], successfully authenticated, clientUuid: aeae1c26-9aad-41cc-a7f5-35a68546a49a, client name: MC-Client-custom_cluster, client version: 5.4.0
2024-12-12 14:53:35,026 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=4, /172.19.0.3:5701->/172.19.0.1:43012, qualifier=null, endpoint=[172.19.0.1]:43012, remoteUuid=65e4981a-ea57-40d7-a501-f4eee1305865, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 65e4981a-ea57-40d7-a501-f4eee1305865, client name: hz.client_0, client version: 5.5.0
2024-12-12 14:53:35,071 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=4, /172.19.0.3:5701->/172.19.0.1:43012, qualifier=null, endpoint=[172.19.0.1]:43012, remoteUuid=65e4981a-ea57-40d7-a501-f4eee1305865, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 14:53:35,092 [ INFO] [hz.festive_germain.event-3] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=4, /172.19.0.3:5701->/172.19.0.1:43012, qualifier=null, endpoint=[172.19.0.1]:43012, remoteUuid=65e4981a-ea57-40d7-a501-f4eee1305865, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=65e4981a-ea57-40d7-a501-f4eee1305865, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015215025, latest clientAttributes=null, labels=[]}
2024-12-12 14:53:57,544 [ INFO] [hz.festive_germain.generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=5, /172.19.0.3:5701->/172.19.0.1:43610, qualifier=null, endpoint=[172.19.0.1]:43610, remoteUuid=592e31c9-4860-4ba2-bca1-28caab7a22b7, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 592e31c9-4860-4ba2-bca1-28caab7a22b7, client name: hz.client_0, client version: 5.5.0
2024-12-12 14:55:42,942 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=5, /172.19.0.3:5701->172.19.0.1/172.19.0.1:43610, qualifier=null, endpoint=[172.19.0.1]:43610, remoteUuid=592e31c9-4860-4ba2-bca1-28caab7a22b7, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 14:55:42,947 [ INFO] [hz.festive_germain.event-5] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=5, /172.19.0.3:5701->172.19.0.1/172.19.0.1:43610, qualifier=null, endpoint=[172.19.0.1]:43610, remoteUuid=592e31c9-4860-4ba2-bca1-28caab7a22b7, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=592e31c9-4860-4ba2-bca1-28caab7a22b7, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015237544, latest clientAttributes=null, labels=[]}
2024-12-12 14:56:02,442 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=6, /172.19.0.3:5701->/172.19.0.1:49028, qualifier=null, endpoint=[172.19.0.1]:49028, remoteUuid=c5be4d4d-cb88-43c6-9d85-ce890485a734, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: c5be4d4d-cb88-43c6-9d85-ce890485a734, client name: hz.client_0, client version: 5.5.0
2024-12-12 14:57:46,314 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=6, /172.19.0.3:5701->172.19.0.1/172.19.0.1:49028, qualifier=null, endpoint=[172.19.0.1]:49028, remoteUuid=c5be4d4d-cb88-43c6-9d85-ce890485a734, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 14:57:46,314 [ INFO] [hz.festive_germain.event-2] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=6, /172.19.0.3:5701->172.19.0.1/172.19.0.1:49028, qualifier=null, endpoint=[172.19.0.1]:49028, remoteUuid=c5be4d4d-cb88-43c6-9d85-ce890485a734, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=c5be4d4d-cb88-43c6-9d85-ce890485a734, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015362442, latest clientAttributes=null, labels=[]}
2024-12-12 14:58:06,939 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=7, /172.19.0.3:5701->/172.19.0.1:36594, qualifier=null, endpoint=[172.19.0.1]:36594, remoteUuid=0d9baf6a-4775-4a67-ae4c-5d8ed8e8e28c, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 0d9baf6a-4775-4a67-ae4c-5d8ed8e8e28c, client name: hz.client_0, client version: 5.5.0
2024-12-12 15:00:06,616 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] The HealthMonitor has detected a high load on the system. For more detailed information,
enable Diagnostics by adding the property -Dhazelcast.diagnostics.enabled=true
2024-12-12 15:00:06,629 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=157.8M, swap.space.total=1024.0M, swap.space.free=635.8M, heap.memory.used=59.8M, heap.memory.free=71.8M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=45.31%, heap.memory.used/max=1.91%, minor.gc.count=24, minor.gc.time=123ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=20, unknown.gc.time=81ms, load.process=0.65%, load.system=74.87%, load.systemAverage=2.36, thread.count=60, thread.peakCount=60, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=127991, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 15:00:08,422 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=7, /172.19.0.3:5701->172.19.0.1/172.19.0.1:36594, qualifier=null, endpoint=[172.19.0.1]:36594, remoteUuid=0d9baf6a-4775-4a67-ae4c-5d8ed8e8e28c, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 15:00:08,422 [ INFO] [hz.festive_germain.event-1] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=7, /172.19.0.3:5701->172.19.0.1/172.19.0.1:36594, qualifier=null, endpoint=[172.19.0.1]:36594, remoteUuid=0d9baf6a-4775-4a67-ae4c-5d8ed8e8e28c, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=0d9baf6a-4775-4a67-ae4c-5d8ed8e8e28c, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015486938, latest clientAttributes=null, labels=[]}
2024-12-12 15:01:06,631 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=161.7M, swap.space.total=1024.0M, swap.space.free=635.8M, heap.memory.used=79.3M, heap.memory.free=52.3M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=60.08%, heap.memory.used/max=2.53%, minor.gc.count=24, minor.gc.time=123ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=20, unknown.gc.time=81ms, load.process=0.32%, load.system=70.43%, load.systemAverage=3.00, thread.count=59, thread.peakCount=61, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=129396, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=1, connection.active.count=3, client.connection.count=1, connection.count=3
2024-12-12 15:02:51,757 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=8, /172.19.0.3:5701->/172.19.0.1:42662, qualifier=null, endpoint=[172.19.0.1]:42662, remoteUuid=2c0a9d70-4b81-4dc1-a38c-aa48d1799d5d, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 2c0a9d70-4b81-4dc1-a38c-aa48d1799d5d, client name: hz.client_0, client version: 5.5.0
2024-12-12 15:04:46,641 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=251.7M, swap.space.total=1024.0M, swap.space.free=613.5M, heap.memory.used=102.6M, heap.memory.free=29.0M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=77.75%, heap.memory.used/max=3.27%, minor.gc.count=26, minor.gc.time=137ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=22, unknown.gc.time=100ms, load.process=0.73%, load.system=74.37%, load.systemAverage=3.63, thread.count=61, thread.peakCount=61, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=216236, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 15:05:12,887 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=8, /172.19.0.3:5701->172.19.0.1/172.19.0.1:42662, qualifier=null, endpoint=[172.19.0.1]:42662, remoteUuid=2c0a9d70-4b81-4dc1-a38c-aa48d1799d5d, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 15:05:12,887 [ INFO] [hz.festive_germain.event-3] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=8, /172.19.0.3:5701->172.19.0.1/172.19.0.1:42662, qualifier=null, endpoint=[172.19.0.1]:42662, remoteUuid=2c0a9d70-4b81-4dc1-a38c-aa48d1799d5d, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=2c0a9d70-4b81-4dc1-a38c-aa48d1799d5d, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015771756, latest clientAttributes=null, labels=[]}
2024-12-12 15:05:39,012 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=9, /172.19.0.3:5701->/172.19.0.1:57450, qualifier=null, endpoint=[172.19.0.1]:57450, remoteUuid=187348af-c46c-41d3-8da6-3bb19f3c14e3, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 187348af-c46c-41d3-8da6-3bb19f3c14e3, client name: hz.client_0, client version: 5.5.0
2024-12-12 15:05:46,646 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=261.0M, swap.space.total=1024.0M, swap.space.free=613.7M, heap.memory.used=87.7M, heap.memory.free=43.9M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=66.46%, heap.memory.used/max=2.80%, minor.gc.count=27, minor.gc.time=145ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=22, unknown.gc.time=100ms, load.process=4.32%, load.system=76.46%, load.systemAverage=4.00, thread.count=59, thread.peakCount=61, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=242803, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=10, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 15:11:06,663 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=242.3M, swap.space.total=1024.0M, swap.space.free=614.0M, heap.memory.used=115.8M, heap.memory.free=15.8M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=87.73%, heap.memory.used/max=3.69%, minor.gc.count=29, minor.gc.time=160ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=24, unknown.gc.time=105ms, load.process=0.63%, load.system=73.49%, load.systemAverage=4.84, thread.count=58, thread.peakCount=64, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=291619, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=10, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 15:26:06,721 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=283.7M, swap.space.total=1024.0M, swap.space.free=614.7M, heap.memory.used=86.0M, heap.memory.free=45.6M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=65.16%, heap.memory.used/max=2.74%, minor.gc.count=36, minor.gc.time=217ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=28, unknown.gc.time=126ms, load.process=0.71%, load.system=70.77%, load.systemAverage=3.18, thread.count=60, thread.peakCount=64, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=430411, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=10, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 15:26:26,724 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=227.6M, swap.space.total=1024.0M, swap.space.free=614.7M, heap.memory.used=96.1M, heap.memory.free=35.5M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=72.82%, heap.memory.used/max=3.06%, minor.gc.count=36, minor.gc.time=217ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=28, unknown.gc.time=126ms, load.process=1.31%, load.system=70.53%, load.systemAverage=2.98, thread.count=58, thread.peakCount=64, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=433519, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=10, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:03:12,212 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=9, /172.19.0.3:5701->172.19.0.1/172.19.0.1:57450, qualifier=null, endpoint=[172.19.0.1]:57450, remoteUuid=187348af-c46c-41d3-8da6-3bb19f3c14e3, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:03:12,213 [ INFO] [hz.festive_germain.event-5] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=9, /172.19.0.3:5701->172.19.0.1/172.19.0.1:57450, qualifier=null, endpoint=[172.19.0.1]:57450, remoteUuid=187348af-c46c-41d3-8da6-3bb19f3c14e3, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=187348af-c46c-41d3-8da6-3bb19f3c14e3, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734015939012, latest clientAttributes=null, labels=[]}
2024-12-12 16:03:13,252 [ WARN] [hz.festive_germain.cached.thread-1] [c.h.c.i.ClientEngine]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No connection left to client cluster wide 187348af-c46c-41d3-8da6-3bb19f3c14e3 for 1106 millis, cleaning resources of the client
2024-12-12 16:03:13,255 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.t.TransactionManagerService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Committing/rolling-back live transactions of client, UUID: 187348af-c46c-41d3-8da6-3bb19f3c14e3
2024-12-12 16:04:26,821 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=257.2M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=110.2M, heap.memory.free=21.4M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=83.49%, heap.memory.used/max=3.51%, minor.gc.count=52, minor.gc.time=279ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.29%, load.system=74.70%, load.systemAverage=2.86, thread.count=60, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=779446, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=1, connection.active.count=3, client.connection.count=1, connection.count=3
2024-12-12 16:04:31,933 [ INFO] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=10, /172.19.0.3:5701->/172.19.0.1:50906, qualifier=null, endpoint=[172.19.0.1]:50906, remoteUuid=015b14c3-8fc0-41ae-8feb-adda64273127, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: 015b14c3-8fc0-41ae-8feb-adda64273127, client name: hz.client_0, client version: 5.5.0
2024-12-12 16:04:46,825 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=240.6M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=117.1M, heap.memory.free=14.5M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=88.74%, heap.memory.used/max=3.73%, minor.gc.count=52, minor.gc.time=279ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.20%, load.system=75.57%, load.systemAverage=2.76, thread.count=62, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=779823, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:05:06,834 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=222.8M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=54.1M, heap.memory.free=77.5M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=41.01%, heap.memory.used/max=1.72%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.10%, load.system=75.44%, load.systemAverage=3.53, thread.count=60, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=780022, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:05:26,838 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=227.7M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=60.3M, heap.memory.free=71.4M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=45.66%, heap.memory.used/max=1.92%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.10%, load.system=78.36%, load.systemAverage=4.25, thread.count=58, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=780237, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:05:46,839 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=222.2M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=66.7M, heap.memory.free=64.9M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=50.55%, heap.memory.used/max=2.13%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.10%, load.system=72.59%, load.systemAverage=4.18, thread.count=58, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=780613, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:06:06,841 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=223.4M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=72.7M, heap.memory.free=59.0M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=55.07%, heap.memory.used/max=2.32%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.29%, load.system=74.85%, load.systemAverage=4.18, thread.count=60, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=780811, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:06:26,845 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=223.7M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=78.7M, heap.memory.free=52.9M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=59.63%, heap.memory.used/max=2.51%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.10%, load.system=70.67%, load.systemAverage=4.83, thread.count=61, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=781022, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:06:46,851 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=225.5M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=85.0M, heap.memory.free=46.6M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=64.40%, heap.memory.used/max=2.71%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.29%, load.system=79.22%, load.systemAverage=5.56, thread.count=59, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=781399, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:07:06,855 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=231.7M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=91.0M, heap.memory.free=40.6M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=68.97%, heap.memory.used/max=2.90%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.39%, load.system=78.85%, load.systemAverage=5.34, thread.count=58, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=781597, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:07:46,858 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=209.3M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=103.7M, heap.memory.free=28.0M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=78.53%, heap.memory.used/max=3.30%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.19%, load.system=76.96%, load.systemAverage=5.13, thread.count=60, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=782187, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:08:06,861 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=239.4M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=109.2M, heap.memory.free=22.4M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=82.73%, heap.memory.used/max=3.48%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.20%, load.system=74.57%, load.systemAverage=4.55, thread.count=60, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=782386, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:08:26,863 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=233.9M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=115.3M, heap.memory.free=16.4M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=87.34%, heap.memory.used/max=3.67%, minor.gc.count=53, minor.gc.time=282ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=44, unknown.gc.time=194ms, load.process=0.10%, load.system=74.14%, load.systemAverage=4.64, thread.count=58, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=782601, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:08:46,866 [ INFO] [hz.festive_germain.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=226.9M, swap.space.total=1024.0M, swap.space.free=616.5M, heap.memory.used=52.5M, heap.memory.free=79.1M, heap.memory.total=132.0M, heap.memory.max=3.1G, heap.memory.used/total=39.77%, heap.memory.used/max=1.67%, minor.gc.count=54, minor.gc.time=288ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=46, unknown.gc.time=198ms, load.process=0.29%, load.system=77.01%, load.systemAverage=4.46, thread.count=59, thread.peakCount=65, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=782977, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:08:55,329 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=10, /172.19.0.3:5701->172.19.0.1/172.19.0.1:50906, qualifier=null, endpoint=[172.19.0.1]:50906, remoteUuid=015b14c3-8fc0-41ae-8feb-adda64273127, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:08:55,331 [ INFO] [hz.festive_germain.event-4] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=10, /172.19.0.3:5701->172.19.0.1/172.19.0.1:50906, qualifier=null, endpoint=[172.19.0.1]:50906, remoteUuid=015b14c3-8fc0-41ae-8feb-adda64273127, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=015b14c3-8fc0-41ae-8feb-adda64273127, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734019471932, latest clientAttributes=null, labels=[]}
2024-12-12 16:17:18,367 [ WARN] [hz.festive_germain.InvocationMonitorThread] [c.h.s.i.o.i.InvocationMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] MonitorInvocationsTask delayed 40056 ms
2024-12-12 16:17:18,368 [ WARN] [hz.festive_germain.cached.thread-7] [c.h.i.c.i.ClusterHeartbeatManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Resetting heartbeat timestamps because of huge system clock jump! Clock-Jump: 40141 ms, Heartbeat-Timeout: 60000 ms
2024-12-12 16:17:18,390 [ WARN] [hz.festive_germain.InvocationMonitorThread] [c.h.s.i.o.i.InvocationMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] BroadcastOperationControlTask delayed 30084 ms
2024-12-12 16:17:18,412 [ WARN] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.i.c.i.ClusterHeartbeatManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignoring heartbeat from Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf since it is expired (now: 2024-12-12 16:17:18.359, timestamp: 2024-12-12 16:16:46.406)
2024-12-12 16:17:18,417 [ WARN] [hz.festive_germain.priority-generic-operation.thread-0] [c.h.i.c.i.ClusterHeartbeatManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignoring heartbeat from Member [192.168.0.104]:5703 - 51bbddca-3e20-4ed9-aec3-8afedc4a147b since it is expired (now: 2024-12-12 16:17:18.413, timestamp: 2024-12-12 16:16:44.561)
2024-12-12 16:17:18,419 [ WARN] [hz.festive_germain.generic-operation.thread-0] [c.h.i.c.i.ClusterHeartbeatManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignoring heartbeat from Member [192.168.0.104]:5703 - 51bbddca-3e20-4ed9-aec3-8afedc4a147b since it is expired (now: 2024-12-12 16:17:18.419, timestamp: 2024-12-12 16:16:39.560)
2024-12-12 16:17:18,423 [ WARN] [hz.festive_germain.generic-operation.thread-1] [c.h.i.c.i.ClusterHeartbeatManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignoring heartbeat from Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf since it is expired (now: 2024-12-12 16:17:18.422, timestamp: 2024-12-12 16:16:41.406)
2024-12-12 16:25:03,808 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=2, /172.19.0.3:5701->/172.19.0.1:58304, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=51bbddca-3e20-4ed9-aec3-8afedc4a147b, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side
2024-12-12 16:25:03,881 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:03,908 [ WARN] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=11, /172.19.0.3:60277->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=11, /172.19.0.3:60277->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-1
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:04,033 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,039 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=12, /172.19.0.3:57287->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,043 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,046 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=13, /172.19.0.3:47807->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,054 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,061 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=14, /172.19.0.3:51651->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,095 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,100 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=15, /172.19.0.3:36373->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,104 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,105 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=16, /172.19.0.3:41309->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,105 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,107 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=17, /172.19.0.3:50003->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,122 [ INFO] [hz.festive_germain.cached.thread-23] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,125 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=18, /172.19.0.3:38065->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,127 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,130 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=19, /172.19.0.3:47787->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,133 [ INFO] [hz.festive_germain.cached.thread-23] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,138 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=20, /172.19.0.3:40267->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,141 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,147 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=21, /172.19.0.3:48247->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,180 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,182 [ WARN] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=22, /172.19.0.3:55839->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=22, /172.19.0.3:55839->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-0
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:04,184 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,185 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=23, /172.19.0.3:40903->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,188 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,190 [ WARN] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=24, /172.19.0.3:33027->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=24, /172.19.0.3:33027->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-2
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioPipeline.lambda$start$0(NioPipeline.java:127) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processTaskQueue(NioThread.java:355) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:290) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:04,202 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,203 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=25, /172.19.0.3:46703->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,204 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,205 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=26, /172.19.0.3:34811->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,207 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,210 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=27, /172.19.0.3:42913->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,213 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,214 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=28, /172.19.0.3:43771->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,215 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,216 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=29, /172.19.0.3:53633->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,218 [ INFO] [hz.festive_germain.cached.thread-17] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,221 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=30, /172.19.0.3:50635->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,234 [ WARN] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnectionErrorHandler]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Removing connection to endpoint [192.168.0.104]:5703 Cause => java.io.EOFException {Remote socket closed!}, Error-Count: 5
2024-12-12 16:25:04,246 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.c.i.MembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Removing Member [192.168.0.104]:5703 - 51bbddca-3e20-4ed9-aec3-8afedc4a147b
2024-12-12 16:25:04,248 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.p.i.PartitionStateManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Storing snapshot of partition assignments while removing UUID 51bbddca-3e20-4ed9-aec3-8afedc4a147b
2024-12-12 16:25:04,237 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5703, timeout: 10000, bind-any: true
2024-12-12 16:25:04,252 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=31, /172.19.0.3:60443->/192.168.0.104:5703, qualifier=null, endpoint=[192.168.0.104]:5703, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:04,256 [ INFO] [hz.festive_germain.cached.thread-7] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:2, ver:4} [
        Member [192.168.0.104]:5701 - 9704bf18-1cec-4db4-8d98-a9a5f55fdf00 this
        Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf
]

2024-12-12 16:25:04,290 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.t.TransactionManagerService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Committing/rolling-back live transactions of [192.168.0.104]:5703, UUID: 51bbddca-3e20-4ed9-aec3-8afedc4a147b
2024-12-12 16:25:04,511 [ INFO] [hz.festive_germain.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Repartitioning cluster data. Migration tasks count: 180
2024-12-12 16:25:05,450 [ INFO] [hz.festive_germain.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] All migration tasks have been completed. (repartitionTime=Thu Dec 12 16:25:04 GMT 2024, plannedMigrations=180, completedMigrations=180, remainingMigrations=0, totalCompletedMigrations=451)
2024-12-12 16:25:06,599 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=1, /172.19.0.3:5701->/172.19.0.1:38174, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=42326535-0b24-4a92-b7cf-cf8058d12bbf, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,604 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,621 [ WARN] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=32, /172.19.0.3:42753->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=32, /172.19.0.3:42753->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-1
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:06,630 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,636 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=33, /172.19.0.3:52215->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,642 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,645 [ WARN] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=34, /172.19.0.3:36121->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=34, /172.19.0.3:36121->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-0
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:06,647 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,650 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=35, /172.19.0.3:60557->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,652 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,655 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=36, /172.19.0.3:55139->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,657 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,658 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=37, /172.19.0.3:43787->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,660 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,661 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=38, /172.19.0.3:33679->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,661 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,662 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=39, /172.19.0.3:57199->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,664 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,666 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=40, /172.19.0.3:41117->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,666 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,668 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=41, /172.19.0.3:33999->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,669 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,670 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=42, /172.19.0.3:33287->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,671 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,672 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=43, /172.19.0.3:46387->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,673 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,683 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=44, /172.19.0.3:55575->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,685 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,689 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=45, /172.19.0.3:53307->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,694 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,695 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=46, /172.19.0.3:42565->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,697 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,705 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=47, /172.19.0.3:49881->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,706 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,707 [ WARN] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=48, /172.19.0.3:42493->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=48, /172.19.0.3:42493->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-2
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:06,710 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,711 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=49, /172.19.0.3:46145->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,713 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,721 [ WARN] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=50, /172.19.0.3:43069->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=50, /172.19.0.3:43069->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-1
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:06,727 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,730 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=51, /172.19.0.3:51901->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,732 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,734 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=52, /172.19.0.3:57859->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,736 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,738 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=53, /172.19.0.3:56247->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,740 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,741 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=54, /172.19.0.3:44927->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,742 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,745 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=55, /172.19.0.3:48805->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,751 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,753 [ INFO] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=56, /172.19.0.3:35977->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,756 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,757 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=57, /172.19.0.3:46911->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,763 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,766 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=58, /172.19.0.3:56125->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,768 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,770 [ WARN] [hz.festive_germain.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=59, /172.19.0.3:55695->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Exception in Connection[id=59, /172.19.0.3:55695->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=true, connectionType=NONE, planeIndex=-1], thread=hz.festive_germain.IO.thread-in-1
java.net.SocketException: Connection reset
        at java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:401) ~[?:?]
        at java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:434) ~[?:?]
        at com.hazelcast.internal.networking.nio.NioInboundPipeline.process(NioInboundPipeline.java:115) ~[hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKey(NioThread.java:383) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.processSelectionKeys(NioThread.java:368) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:294) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249) [hazelcast-5.4.0.jar:5.4.0]
        at com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111) [hazelcast-5.4.0.jar:5.4.0]
2024-12-12 16:25:06,774 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,775 [ INFO] [hz.festive_germain.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=60, /172.19.0.3:54315->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,776 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,779 [ INFO] [hz.festive_germain.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=61, /172.19.0.3:33803->/192.168.0.104:5702, qualifier=null, endpoint=[192.168.0.104]:5702, remoteUuid=null, alive=false, connectionType=NONE, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:25:06,782 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,783 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,784 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,784 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,794 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,795 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,797 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,797 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,806 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,807 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,813 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,813 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,825 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,825 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,832 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,833 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,836 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,837 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,839 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,840 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,842 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,843 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,852 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,853 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,855 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,856 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,864 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,866 [ INFO] [hz.festive_germain.cached.thread-1] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,867 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,868 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,874 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,874 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,876 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,877 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,883 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,884 [ INFO] [hz.festive_germain.cached.thread-2] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,885 [ INFO] [hz.festive_germain.cached.thread-23] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,886 [ INFO] [hz.festive_germain.cached.thread-23] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,900 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,901 [ INFO] [hz.festive_germain.cached.thread-18] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,908 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connecting to /192.168.0.104:5702, timeout: 10000, bind-any: true
2024-12-12 16:25:06,909 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnector]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Could not connect to: /192.168.0.104:5702. Reason: IOException[Connection refused to address /192.168.0.104:5702]
2024-12-12 16:25:06,909 [ WARN] [hz.festive_germain.cached.thread-9] [c.h.i.s.t.TcpServerConnectionErrorHandler]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Removing connection to endpoint [192.168.0.104]:5702 Cause => java.io.IOException {Connection refused to address /192.168.0.104:5702}, Error-Count: 5
2024-12-12 16:25:06,909 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.c.i.MembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Removing Member [192.168.0.104]:5702 - 42326535-0b24-4a92-b7cf-cf8058d12bbf
2024-12-12 16:25:06,911 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.p.i.PartitionStateManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Storing snapshot of partition assignments while removing UUID 42326535-0b24-4a92-b7cf-cf8058d12bbf
2024-12-12 16:25:06,911 [ INFO] [hz.festive_germain.cached.thread-9] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:1, ver:5} [
        Member [192.168.0.104]:5701 - 9704bf18-1cec-4db4-8d98-a9a5f55fdf00 this
]

2024-12-12 16:25:06,911 [ INFO] [hz.festive_germain.cached.thread-23] [c.h.t.TransactionManagerService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Committing/rolling-back live transactions of [192.168.0.104]:5702, UUID: 42326535-0b24-4a92-b7cf-cf8058d12bbf
2024-12-12 16:25:06,949 [ INFO] [hz.festive_germain.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Partition balance is ok, no need to repartition.
^C2024-12-12 16:25:08,141 [ INFO] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Running shutdown hook... Current node state: ACTIVE
2024-12-12 16:25:08,141 [ INFO] [hz.ShutdownThread] [c.h.c.LifecycleService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] [192.168.0.104]:5701 is SHUTTING_DOWN
2024-12-12 16:25:08,142 [ WARN] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Terminating forcefully...
2024-12-12 16:25:08,142 [ INFO] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Shutting down multicast service...
2024-12-12 16:25:08,148 [ INFO] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Shutting down connection manager...
2024-12-12 16:25:08,149 [ INFO] [hz.ShutdownThread] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=3, /172.19.0.3:5701->172.19.0.1/172.19.0.1:40942, qualifier=null, endpoint=[172.19.0.1]:40942, remoteUuid=aeae1c26-9aad-41cc-a7f5-35a68546a49a, alive=false, connectionType=MCJVM, planeIndex=-1] closed. Reason: TcpServer is stopping
2024-12-12 16:25:08,151 [ INFO] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Shutting down node engine...
2024-12-12 16:25:08,202 [ INFO] [hz.ShutdownThread] [c.h.i.i.NodeExtension]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying node NodeExtension.
2024-12-12 16:25:08,204 [ INFO] [hz.ShutdownThread] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Hazelcast Shutdown is completed in 60 ms.
                                                                                                                                                                                                                                            
â”Œâ”€â”€(ivanã‰¿kali)-[~/pvns/1]
â””â”€$ sudo docker run -it --name node_first --network lab1 --rm -e HZ_NETWORK_PUBLICADDRESS=192.168.0.104:5701 -e HZ_CLUSTERNAME=custom_cluster -p 5701:5701 hazelcast/node_first_bu
[sudo] password for ivan: 
########################################
# JAVA=/usr/bin/java
# JAVA_OPTS=--add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED -Dlog4j.shutdownHookEnabled=false -Dhazelcast.logging.shutdown=true -Dhazelcast.logging.type=log4j2 -Dlog4j.configurationFile=file:/opt/hazelcast/config/log4j2.properties -Dhazelcast.config=/opt/hazelcast/config/hazelcast-docker.xml -Djet.custom.lib.dir=/opt/hazelcast/custom-lib -Djava.net.preferIPv4Stack=true -XX:MaxRAMPercentage=80.0
# CLASSPATH=/opt/hazelcast/*:/opt/hazelcast/lib:/opt/hazelcast/lib/*:/opt/hazelcast/bin/user-lib:/opt/hazelcast/bin/user-lib/*
########################################
2024-12-12 16:26:05,088 [ INFO] [main] [c.h.i.c.AbstractConfigLocator]: Loading configuration '/opt/hazelcast/config/hazelcast-docker.xml' from System property 'hazelcast.config'
2024-12-12 16:26:05,091 [ INFO] [main] [c.h.i.c.AbstractConfigLocator]: Using configuration file at /opt/hazelcast/config/hazelcast-docker.xml
2024-12-12 16:26:05,304 [ WARN] [main] [c.h.c.AbstractXmlConfigHelper]: Name of the hazelcast schema location[http://www.hazelcast.com/schema/config/hazelcast-config-5.3.xsd] is incorrect, using default
2024-12-12 16:26:05,664 [ INFO] [main] [c.h.i.c.o.ExternalConfigurationOverride]: Detected external configuration entries in environment variables: [hazelcast.network.publicaddress=192.168.0.104:5701,hazelcast.clustername=custom_cluster]
2024-12-12 16:26:05,737 [ INFO] [main] [c.h.i.AddressPicker]: [LOCAL] [custom_cluster] [5.4.0] Using public address: [192.168.0.104]:5701
2024-12-12 16:26:05,798 [ INFO] [main] [c.h.s.logo]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 
        o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
        |    |    / \       /         |     /         / \    |         |   
        o----o       o     o   o----o |    o             o   o----o    |   
        |    |  *     \   /           |     \       *     \       |    |   
        o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2024-12-12 16:26:05,798 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2024-12-12 16:26:05,801 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [192.168.0.104]:5701
2024-12-12 16:26:05,801 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Cluster name: custom_cluster
2024-12-12 16:26:05,801 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2024-12-12 16:26:05,801 [ INFO] [main] [c.h.system]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Jet is enabled
2024-12-12 16:26:06,589 [ INFO] [main] [c.h.s.security]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see ðŸ”’ security recommendations and the status of current config.
2024-12-12 16:26:06,657 [ INFO] [main] [c.h.i.i.Node]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Using Multicast discovery
2024-12-12 16:26:06,661 [ INFO] [main] [c.h.c.CPSubsystem]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is enabled with 3 members.
2024-12-12 16:26:06,932 [ INFO] [main] [c.h.j.i.JetServiceBackend]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Setting number of cooperative threads and default parallelism to 4
2024-12-12 16:26:07,392 [ INFO] [main] [c.h.i.d.Diagnostics]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2024-12-12 16:26:07,397 [ INFO] [main] [c.h.c.LifecycleService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] [192.168.0.104]:5701 is STARTING
2024-12-12 16:26:09,582 [ INFO] [main] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:1, ver:1} [
        Member [192.168.0.104]:5701 - 39ceb316-571f-4eef-9d6e-1ba1c51f4f25 this
]

2024-12-12 16:26:09,589 [ INFO] [main] [c.h.j.i.JobCoordinationService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Jet started scanning for jobs
2024-12-12 16:26:09,592 [ INFO] [main] [c.h.c.LifecycleService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] [192.168.0.104]:5701 is STARTED
2024-12-12 16:26:09,616 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.i.p.i.PartitionStateManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initializing cluster partition table arrangement...
2024-12-12 16:26:09,867 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 1
2024-12-12 16:26:11,215 [ INFO] [hz.inspiring_dewdney.priority-generic-operation.thread-0] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=1, /172.19.0.3:5701->/172.19.0.1:58574, qualifier=null, endpoint=[172.19.0.1]:58574, remoteUuid=aeae1c26-9aad-41cc-a7f5-35a68546a49a, alive=true, connectionType=MCJVM, planeIndex=-1], successfully authenticated, clientUuid: aeae1c26-9aad-41cc-a7f5-35a68546a49a, client name: MC-Client-custom_cluster, client version: 5.4.0
2024-12-12 16:26:14,874 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 1
2024-12-12 16:26:19,880 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 1
2024-12-12 16:26:24,885 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 1
2024-12-12 16:26:29,595 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] The HealthMonitor has detected a high load on the system. For more detailed information,
enable Diagnostics by adding the property -Dhazelcast.diagnostics.enabled=true
2024-12-12 16:26:29,607 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=711.8M, swap.space.total=1024.0M, swap.space.free=656.7M, heap.memory.used=71.7M, heap.memory.free=25.9M, heap.memory.total=98.0M, heap.memory.max=3.1G, heap.memory.used/total=73.15%, heap.memory.used/max=2.28%, minor.gc.count=11, minor.gc.time=60ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=8, unknown.gc.time=9ms, load.process=1.62%, load.system=76.73%, load.systemAverage=2.14, thread.count=54, thread.peakCount=54, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=40, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=1, proxy.count=4, clientEndpoint.count=1, connection.active.count=1, client.connection.count=1, connection.count=1
2024-12-12 16:26:29,889 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 1
2024-12-12 16:26:30,289 [ INFO] [hz.inspiring_dewdney.IO.thread-in-1] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initialized new cluster connection between /172.19.0.3:5701 and /172.19.0.1:41590
2024-12-12 16:26:30,383 [ INFO] [hz.inspiring_dewdney.generic-operation.thread-1] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:2, ver:2} [
        Member [192.168.0.104]:5701 - 39ceb316-571f-4eef-9d6e-1ba1c51f4f25 this
        Member [192.168.0.104]:5702 - a06cdd4a-94af-4c23-83e0-d93a9dd7f299
]

2024-12-12 16:26:30,647 [ INFO] [hz.inspiring_dewdney.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Repartitioning cluster data. Migration tasks count: 271
2024-12-12 16:26:31,723 [ INFO] [hz.inspiring_dewdney.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] All migration tasks have been completed. (repartitionTime=Thu Dec 12 16:26:30 GMT 2024, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=271)
2024-12-12 16:26:34,893 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 2
2024-12-12 16:26:39,898 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 2
2024-12-12 16:26:44,902 [ INFO] [hz.inspiring_dewdney.cached.thread-5] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is waiting for 3 members to join the cluster. Current member count: 2
2024-12-12 16:26:47,837 [ INFO] [hz.inspiring_dewdney.IO.thread-in-2] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Initialized new cluster connection between /172.19.0.3:5701 and /172.19.0.1:57396
2024-12-12 16:26:47,921 [ INFO] [hz.inspiring_dewdney.priority-generic-operation.thread-0] [c.h.i.c.ClusterService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

Members {size:3, ver:3} [
        Member [192.168.0.104]:5701 - 39ceb316-571f-4eef-9d6e-1ba1c51f4f25 this
        Member [192.168.0.104]:5702 - a06cdd4a-94af-4c23-83e0-d93a9dd7f299
        Member [192.168.0.104]:5703 - b0d6a455-60f8-4d10-9848-453467510a43
]

2024-12-12 16:26:48,180 [ INFO] [hz.inspiring_dewdney.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Repartitioning cluster data. Migration tasks count: 271
2024-12-12 16:26:48,918 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftInvocationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Replaced CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=-1}, members=[]} with CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=0}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]}
2024-12-12 16:26:48,954 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] RaftNode[CPGroupId{name='METADATA', seed=0, groupId=0}] is created with [RaftEndpoint{uuid='a06cdd4a-94af-4c23-83e0-d93a9dd7f299'}, RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, RaftEndpoint{uuid='39ceb316-571f-4eef-9d6e-1ba1c51f4f25'}]
2024-12-12 16:26:48,957 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Status is set to: ACTIVE
2024-12-12 16:26:49,284 [ INFO] [hz.inspiring_dewdney.migration] [c.h.i.p.i.MigrationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] All migration tasks have been completed. (repartitionTime=Thu Dec 12 16:26:48 GMT 2024, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=542)
2024-12-12 16:26:49,611 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=383.9M, swap.space.total=1024.0M, swap.space.free=657.0M, heap.memory.used=69.2M, heap.memory.free=28.4M, heap.memory.total=98.0M, heap.memory.max=3.1G, heap.memory.used/total=70.63%, heap.memory.used/max=2.21%, minor.gc.count=12, minor.gc.time=69ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=8, unknown.gc.time=9ms, load.process=12.43%, load.system=72.11%, load.systemAverage=2.75, thread.count=60, thread.peakCount=60, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=887, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=2, proxy.count=4, clientEndpoint.count=1, connection.active.count=3, client.connection.count=1, connection.count=3
2024-12-12 16:26:51,099 [ WARN] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNodeImpl$LeaderFailureDetectionTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] We are FOLLOWER and there is no current leader. Will start new election round...
2024-12-12 16:26:51,109 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.t.PreVoteTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Pre-vote started for next term: 1, last log index: 0, last log term: 0
2024-12-12 16:26:51,110 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

CP Group Members {groupId: METADATA(0), size:3, term:0, logIndex:0} [
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} - FOLLOWER this
]

2024-12-12 16:26:51,137 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.PreVoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Pre-vote granted from RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'} for term: 1, number of votes: 2, majority: 2
2024-12-12 16:26:51,141 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.PreVoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] We have the majority during pre-vote phase. Let's start real election!
2024-12-12 16:26:51,142 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.t.LeaderElectionTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Leader election started for term: 1, last log index: 0, last log term: 0
2024-12-12 16:26:51,146 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

CP Group Members {groupId: METADATA(0), size:3, term:1, logIndex:0} [
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} - CANDIDATE this
]

2024-12-12 16:26:51,165 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.PreVoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignored PreVoteResponse{voter=RaftEndpoint{uuid='a06cdd4a-94af-4c23-83e0-d93a9dd7f299'}, term=1, granted=true}. We are not FOLLOWER anymore.
2024-12-12 16:26:51,169 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.VoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Vote granted from RaftEndpoint{uuid='a06cdd4a-94af-4c23-83e0-d93a9dd7f299'} for term: 1, number of votes: 2, majority: 2
2024-12-12 16:26:51,173 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.VoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] We are the LEADER!
2024-12-12 16:26:51,176 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

CP Group Members {groupId: METADATA(0), size:3, term:1, logIndex:0} [
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} - LEADER this
]

2024-12-12 16:26:51,183 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.VoteResponseHandlerTask(METADATA)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Ignored VoteResponse{voter=RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, term=1, granted=true}. We are not CANDIDATE anymore.
2024-12-12 16:26:51,306 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.RaftInvocationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Replaced CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=0}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]} with CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=2}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]}
2024-12-12 16:26:51,474 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.RaftInvocationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Replaced CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=2}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]} with CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=3}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]}
2024-12-12 16:26:51,573 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.RaftInvocationManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Replaced CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=3}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]} with CPMembersContainer{version=CPMembersVersion{groupIdSeed=0, version=4}, members=[CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]}
2024-12-12 16:26:51,576 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CP Subsystem is initialized with: [CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}, CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}]
2024-12-12 16:27:05,053 [ INFO] [hz.inspiring_dewdney.generic-operation.thread-1] [c.h.c.i.p.t.AuthenticationMessageTask]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Received auth from Connection[id=4, /172.19.0.3:5701->/172.19.0.1:42104, qualifier=null, endpoint=[172.19.0.1]:42104, remoteUuid=fdf244b3-9591-4e13-8c76-97650b243c05, alive=true, connectionType=PYH, planeIndex=-1], successfully authenticated, clientUuid: fdf244b3-9591-4e13-8c76-97650b243c05, client name: hz.client_0, client version: 5.5.0
2024-12-12 16:27:05,108 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.MetadataRaftGroupManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] New CPGroupId{name='default', seed=0, groupId=1634} is created with [CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}, CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}, CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}]
2024-12-12 16:27:05,116 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftService]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] RaftNode[CPGroupId{name='default', seed=0, groupId=1634}] is created with [RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, RaftEndpoint{uuid='39ceb316-571f-4eef-9d6e-1ba1c51f4f25'}, RaftEndpoint{uuid='a06cdd4a-94af-4c23-83e0-d93a9dd7f299'}]
2024-12-12 16:27:05,121 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Status is set to: ACTIVE
2024-12-12 16:27:07,526 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.PreVoteRequestHandlerTask(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Granted pre-vote for PreVoteRequest{candidate=RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, nextTerm=1, lastLogTerm=0, lastLogIndex=0}
2024-12-12 16:27:07,534 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.VoteRequestHandlerTask(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Moving to new term: 1 from current term: 0 after VoteRequest{candidate=RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, term=1, lastLogTerm=0, lastLogIndex=0, disruptive=false}
2024-12-12 16:27:07,535 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

CP Group Members {groupId: default(1634), size:3, term:1, logIndex:0} [
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} - FOLLOWER this
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}
]

2024-12-12 16:27:07,536 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.VoteRequestHandlerTask(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Granted vote for VoteRequest{candidate=RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}, term=1, lastLogTerm=0, lastLogIndex=0, disruptive=false}
2024-12-12 16:27:07,537 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.PreVoteRequestHandlerTask(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Granted pre-vote for PreVoteRequest{candidate=RaftEndpoint{uuid='a06cdd4a-94af-4c23-83e0-d93a9dd7f299'}, nextTerm=1, lastLogTerm=0, lastLogIndex=0}
2024-12-12 16:27:07,558 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.h.AppendRequestHandlerTask(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Setting leader: RaftEndpoint{uuid='b0d6a455-60f8-4d10-9848-453467510a43'}
2024-12-12 16:27:07,559 [ INFO] [hz.inspiring_dewdney.partition-operation.thread-0] [c.h.c.i.r.i.RaftNode(default)]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] 

CP Group Members {groupId: default(1634), size:3, term:1, logIndex:0} [
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} - LEADER
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} - FOLLOWER this
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702}
]

2024-12-12 16:27:09,614 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=317.0M, swap.space.total=1024.0M, swap.space.free=657.0M, heap.memory.used=55.4M, heap.memory.free=42.3M, heap.memory.total=98.0M, heap.memory.max=3.1G, heap.memory.used/total=56.49%, heap.memory.used/max=1.76%, minor.gc.count=12, minor.gc.time=69ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=8, unknown.gc.time=9ms, load.process=15.99%, load.system=89.04%, load.systemAverage=3.79, thread.count=60, thread.peakCount=60, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=1925, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=5, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:27:29,622 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=223.3M, swap.space.total=1024.0M, swap.space.free=657.2M, heap.memory.used=60.1M, heap.memory.free=37.5M, heap.memory.total=98.0M, heap.memory.max=3.1G, heap.memory.used/total=61.38%, heap.memory.used/max=1.92%, minor.gc.count=15, minor.gc.time=100ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=10, unknown.gc.time=24ms, load.process=7.81%, load.system=83.35%, load.systemAverage=4.48, thread.count=59, thread.peakCount=60, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=1, executor.q.priorityOperation.size=0, operations.completed.count=18142, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=1, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=3, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:27:49,629 [ INFO] [hz.inspiring_dewdney.HealthMonitor] [c.h.i.d.HealthMonitor]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] processors=4, physical.memory.total=3.8G, physical.memory.free=269.7M, swap.space.total=1024.0M, swap.space.free=657.2M, heap.memory.used=84.4M, heap.memory.free=13.1M, heap.memory.total=98.0M, heap.memory.max=3.1G, heap.memory.used/total=86.17%, heap.memory.used/max=2.69%, minor.gc.count=17, minor.gc.time=115ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=12, unknown.gc.time=43ms, load.process=7.52%, load.system=75.11%, load.systemAverage=6.56, thread.count=58, thread.peakCount=60, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=40896, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0.00%, operations.pending.invocations.count=3, proxy.count=4, clientEndpoint.count=2, connection.active.count=4, client.connection.count=2, connection.count=4
2024-12-12 16:27:50,307 [ INFO] [hz.inspiring_dewdney.IO.thread-in-0] [c.h.i.s.t.TcpServerConnection]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Connection[id=4, /172.19.0.3:5701->172.19.0.1/172.19.0.1:42104, qualifier=null, endpoint=[172.19.0.1]:42104, remoteUuid=fdf244b3-9591-4e13-8c76-97650b243c05, alive=false, connectionType=PYH, planeIndex=-1] closed. Reason: Connection closed by the other side
2024-12-12 16:27:50,323 [ INFO] [hz.inspiring_dewdney.event-1] [c.h.c.i.ClientEndpointManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Destroying ClientEndpoint{connection=Connection[id=4, /172.19.0.3:5701->172.19.0.1/172.19.0.1:42104, qualifier=null, endpoint=[172.19.0.1]:42104, remoteUuid=fdf244b3-9591-4e13-8c76-97650b243c05, alive=false, connectionType=PYH, planeIndex=-1], clientUuid=fdf244b3-9591-4e13-8c76-97650b243c05, clientName=hz.client_0, authenticated=true, clientVersion=5.5.0, creationTime=1734020825053, latest clientAttributes=null, labels=[]}
2024-12-12 16:27:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:27:51,618 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:27:51,619 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:27:51,621 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:27:51,624 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:27:51,629 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:27:51,629 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:27:51,631 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:27:51,631 [ INFO] [hz.inspiring_dewdney.cached.thread-2] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:28:51,600 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:28:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:28:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:28:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:28:51,610 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:28:51,610 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:28:51,610 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:28:51,613 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:28:51,615 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:29:51,599 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:29:51,601 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:29:51,601 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:29:51,601 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:29:51,602 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:29:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:29:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:29:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:29:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:30:51,600 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:30:51,602 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:30:51,602 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:30:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:30:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:30:51,606 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:30:51,607 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:30:51,609 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:30:51,610 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:31:51,601 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:31:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:31:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:31:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:31:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:31:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:31:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:31:51,607 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:31:51,608 [ INFO] [hz.inspiring_dewdney.cached.thread-4] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:32:51,601 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:32:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:32:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:32:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:32:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:32:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:32:51,605 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:32:51,607 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:32:51,607 [ INFO] [hz.inspiring_dewdney.cached.thread-1] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
2024-12-12 16:33:51,600 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:33:51,602 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:33:51,602 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} with 1 leaderships.
2024-12-12 16:33:51,603 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703}. Skipping to next...
2024-12-12 16:33:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:33:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Searching a candidate transfer leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} with 1 leaderships.
2024-12-12 16:33:51,604 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] No candidate could be found to get leadership from CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701}. Skipping to next...
2024-12-12 16:33:51,606 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] Current leadership claims:
        CPMember{uuid=b0d6a455-60f8-4d10-9848-453467510a43, address=[192.168.0.104]:5703} priority 0 has 1,
        CPMember{uuid=39ceb316-571f-4eef-9d6e-1ba1c51f4f25, address=[192.168.0.104]:5701} priority 0 has 1,
        CPMember{uuid=a06cdd4a-94af-4c23-83e0-d93a9dd7f299, address=[192.168.0.104]:5702} priority 0 has 0, leaderships.
2024-12-12 16:33:51,607 [ INFO] [hz.inspiring_dewdney.cached.thread-3] [c.h.c.i.RaftGroupMembershipManager]: [192.168.0.104]:5701 [custom_cluster] [5.4.0] CPGroup leadership balance is fine, cannot rebalance further...
